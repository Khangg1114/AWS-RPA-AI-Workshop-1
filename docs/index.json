[
{
	"uri": "//localhost:1313/",
	"title": "Simple RPA Platform with AI on AWS",
	"tags": [],
	"description": "",
	"content": "Build a Simple RPA Platform with AI Integration on AWS Overview In this workshop, you will build a simple Robotic Process Automation (RPA) platform enhanced with AI capabilities using AWS services. You\u0026rsquo;ll create a basic automation system that can:\nProcess documents automatically using AI Extract data from invoices and forms Send automated emails Monitor and log all activities Scale based on workload What You\u0026rsquo;ll Build A simple RPA platform with these components:\nDocument Processing Bot: Uses Amazon Textract to extract data from PDFs Email Automation Bot: Sends automated responses using SES Data Processing: Stores and processes data in DynamoDB Monitoring Dashboard: CloudWatch metrics and alarms AWS Services Used AWS Lambda: Serverless functions for RPA bots Amazon Textract: AI document processing Amazon SES: Email automation Amazon DynamoDB: Data storage Amazon S3: File storage Amazon CloudWatch: Monitoring and logging Estimated cost: $5-10. Remember to clean up resources after completion.\nPrerequisites AWS Account with basic permissions Basic knowledge of Python Text editor or IDE Workshop Structure Setup AWS Environment Create Document Processing Bot Build Email Automation Bot Cleanup Resources Learning Objectives By the end of this workshop, you will:\nUnderstand RPA concepts and implementation Know how to integrate AI services with automation Be able to build serverless automation solutions Understand AWS monitoring and logging "
},
{
	"uri": "//localhost:1313/1-setup/",
	"title": "Setup AWS Environment",
	"tags": [],
	"description": "",
	"content": "Overview Set up your AWS environment with the necessary services and permissions for the RPA platform.\nWhat You\u0026rsquo;ll Create S3 bucket for document storage DynamoDB table for data storage IAM roles and policies SES email configuration Implementation Steps Create S3 Bucket Create DynamoDB Table Setup Amazon SES Create IAM Role Step 5: Verify Setup Check that you have created:\n✅ S3 bucket: rpa-documents-[your-name] ✅ DynamoDB table: rpa-processed-data ✅ Verified email in SES ✅ IAM role: RPA-Lambda-Role Expected Result Your AWS environment is ready with:\nStorage for documents and data Email service configured Proper permissions for Lambda functions Write down your S3 bucket name and verified email address - you\u0026rsquo;ll need them in the next steps!\n"
},
{
	"uri": "//localhost:1313/1-setup/1.1-create-s3-bucket/",
	"title": "Create S3 Bucket",
	"tags": [],
	"description": "",
	"content": "Overview Create an Amazon S3 bucket to store documents that will be processed by our RPA system.\nStep-by-Step Instructions Step 1: Navigate to S3 Service Open the AWS Management Console In the search bar, type S3 Click on Amazon S3 from the results Step 2: Create New Bucket Click the Create bucket button You\u0026rsquo;ll see the \u0026ldquo;Create bucket\u0026rdquo; configuration page Step 3: Configure Bucket Settings Bucket name:\nEnter: rpa-documents-[your-name] Replace [your-name] with your actual name or initials Example: rpa-documents-fcj Note: Bucket names must be globally unique AWS Region:\nSelect ap-southeast-1 This region has the best support for all services we\u0026rsquo;ll use Step 4: Configure Options Object Ownership:\nKeep the default: ACLs disabled (recommended) Block Public Access settings:\nKeep all checkboxes checked (this is secure) We don\u0026rsquo;t need public access for this workshop Bucket Versioning:\nKeep Disable (default) Default encryption:\nKeep Amazon S3 managed keys (SSE-S3) (default) Step 5: Create the Bucket Scroll down and click Create bucket You should see a success message Your bucket will appear in the S3 buckets list Step 6: Verify Bucket Creation Find your bucket in the list: rpa-documents-[your-name Click on the bucket name to open it You should see an empty bucket with tabs: Objects, Properties, Permissions, etc. Step 7: Create Folder Structure Inside your bucket, click Create folder Folder name: documents Click Create folder This will help organize uploaded files Write down your exact bucket name - you\u0026rsquo;ll need it in later steps!\n"
},
{
	"uri": "//localhost:1313/1-setup/1.2-create-dynamodb/",
	"title": "Create DynamoDB Table",
	"tags": [],
	"description": "",
	"content": "Overview Create an Amazon DynamoDB table to store the processed document data from our RPA system.\nStep-by-Step Instructions Step 1: Navigate to DynamoDB Service In the AWS Management Console Search for DynamoDB in the search bar Click on Amazon DynamoDB from the results Step 2: Create New Table Click the Create table button You\u0026rsquo;ll see the \u0026ldquo;Create table\u0026rdquo; configuration page Step 3: Configure Table Settings Table name:\nEnter: rpa-processed-data This will store all our processed document information Partition key:\nKey name: document_id Type: String This will be the unique identifier for each processed document Step 4: Table Settings Table settings:\nKeep Default settings selected This will use on-demand billing (pay per use) Secondary indexes:\nLeave empty (we don\u0026rsquo;t need any for this workshop) Encryption at rest:\nKeep Owned by Amazon DynamoDB (default) This provides encryption without extra cost Step 5: Create the Table Scroll down and click Create table You\u0026rsquo;ll see \u0026ldquo;Creating table\u0026hellip;\u0026rdquo; status Wait for the table status to change to Active (usually takes 1-2 minutes) Step 6: Verify Table Creation You should see your table rpa-processed-data in the tables list Click on the table name to view details Check the General information tab: Status should be Active Partition key should be document_id (S) Step 7: Explore Table Structure Click on Explore table items tab You\u0026rsquo;ll see \u0026ldquo;No items to display\u0026rdquo; - this is normal for a new table Note the Actions dropdown - we\u0026rsquo;ll use this later for testing The table name rpa-processed-data will be used in our Lambda functions - remember this exact name!\n"
},
{
	"uri": "//localhost:1313/1-setup/1.3-setup-ses/",
	"title": "Setup Amazon SES",
	"tags": [],
	"description": "",
	"content": "Overview Configure Amazon Simple Email Service (SES) to send automated email notifications from our RPA system.\nStep-by-Step Instructions Step 1: Navigate to Amazon SES In the AWS Management Console Search for SES in the search bar Click on Amazon Simple Email Service from the results Step 2: Verify Your Email Address In the SES console, click Verified identities in the left menu Click the Create identity button You\u0026rsquo;ll see the \u0026ldquo;Create identity\u0026rdquo; configuration page Step 3: Configure Email Identity Identity type:\nSelect Email address (not domain) Email address:\nEnter your personal email address Example: your-email@gmail.com This will be used to send and receive RPA notifications Default configuration set:\nLeave empty (we don\u0026rsquo;t need this for the workshop) Step 4: Create the Identity Click Create identity button You\u0026rsquo;ll see a success message Your email will appear in the identities list with status Unverified Step 5: Verify Your Email Check your email inbox (including spam folder) Look for email from Amazon Web Services Subject: \u0026ldquo;Amazon SES Address Verification Request in region\u0026hellip;\u0026rdquo; Click the verification link in the email You\u0026rsquo;ll see a confirmation page in your browser Step 6: Confirm Verification Go back to the SES console Refresh the page Your email status should now show Verified with a green checkmark If still unverified, wait a few minutes and refresh again Keep your verified email address handy - you\u0026rsquo;ll need to update it in the Lambda function code later!\n"
},
{
	"uri": "//localhost:1313/1-setup/1.4-create-iam-role/",
	"title": "Create IAM Role",
	"tags": [],
	"description": "",
	"content": "Overview Create an IAM role that gives our Lambda functions permission to access S3, DynamoDB, Textract, and SES services.\nStep-by-Step Instructions Step 1: Navigate to IAM Service In the AWS Management Console Search for IAM in the search bar Click on IAM from the results Step 2: Create New Role In the IAM console, click Roles in the left menu Click the Create role button You\u0026rsquo;ll see the \u0026ldquo;Create role\u0026rdquo; page Step 3: Select Trusted Entity Trusted entity type:\nSelect AWS service (should be selected by default) Use case:\nSelect Lambda from the list This allows Lambda functions to assume this role Step 4: Add Permissions Click Next to go to permissions page You\u0026rsquo;ll see a search box to find policies We need to attach multiple policies for our RPA system Search and select these policies one by one:\nAWSLambdaBasicExecutionRole\nSearch: AWSLambdaBasicExecutionRole Check the box next to it This allows Lambda to write logs to CloudWatch AmazonTextractFullAccess\nSearch: AmazonTextractFullAccess Check the box next to it This allows Lambda to use Textract for document processing AmazonSESFullAccess\nSearch: AmazonSESFullAccess Check the box next to it This allows Lambda to send emails via SES AmazonDynamoDBFullAccess\nSearch: AmazonDynamoDBFullAccess Check the box next to it This allows Lambda to read/write DynamoDB tables AmazonS3FullAccess\nSearch: AmazonS3FullAccess Check the box next to it This allows Lambda to read files from S3 Step 5: Review Selected Policies You should have 5 policies selected:\n✅ AWSLambdaBasicExecutionRole ✅ AmazonTextractFullAccess ✅ AmazonSESFullAccess ✅ AmazonDynamoDBFullAccess ✅ AmazonS3FullAccess Click Next to continue. Step 6: Name and Create Role Role name:\nEnter: RPA-Lambda-Role This name will be used when creating Lambda functions Description:\nEnter: Role for RPA Lambda functions with access to S3, DynamoDB, Textract, and SES Tags (optional):\nKey: Project, Value: RPA-Workshop Key: Purpose, Value: Lambda-Execution Step 7: Create the Role Review all settings: Trusted entity: Lambda Policies: 5 policies attached Role name: RPA-Lambda-Role Click Create role You\u0026rsquo;ll see a success message Step 8: Verify Role Creation You should see RPA-Lambda-Role in the roles list Click on the role name to view details Check the Permissions tab - you should see all 5 policies Check the Trust relationships tab - should show Lambda service The role name RPA-Lambda-Role will be used when creating Lambda functions - remember this exact name!\n"
},
{
	"uri": "//localhost:1313/2-document-bot/",
	"title": "Create Document Processing Bot",
	"tags": [],
	"description": "",
	"content": "Overview Build a Lambda function that automatically processes documents uploaded to S3 using Amazon Textract to extract text and data.\nWhat You\u0026rsquo;ll Build Lambda function triggered by S3 uploads Integration with Amazon Textract for AI document processing Automatic data extraction from invoices and forms Implementation Steps Create Lambda Function Add Processing Code Configure S3 Trigger Test Document Processing Expected Result Lambda function processes documents automatically Textract extracts text and form data Results stored in DynamoDB System ready for email automation Try uploading different types of documents to see how Textract handles various formats!\n"
},
{
	"uri": "//localhost:1313/2-document-bot/2.1-create-lambda/",
	"title": "Create Lambda Function",
	"tags": [],
	"description": "",
	"content": "Overview Create the first Lambda function that will automatically process documents uploaded to S3 using Amazon Textract.\nStep-by-Step Instructions Step 1: Navigate to AWS Lambda In the AWS Management Console Search for Lambda in the search bar Click on AWS Lambda from the results Step 2: Create New Function Click the Create function button You\u0026rsquo;ll see the \u0026ldquo;Create function\u0026rdquo; page with different options Step 3: Choose Function Creation Method Function creation method:\nSelect Author from scratch (should be selected by default) This allows us to write our own code Step 4: Configure Basic Information Function name:\nEnter: document-processor This name describes what the function does Runtime:\nSelect Python 3.9 from the dropdown Python is great for AWS integrations and AI services Architecture:\nKeep x86_64 (default) This is the standard architecture Step 5: Configure Permissions Execution role:\nSelect Use an existing role From the dropdown, choose: RPA-Lambda-Role This is the role we created in the previous step Step 6: Advanced Settings (Optional) Enable function URL:\nLeave unchecked (we don\u0026rsquo;t need a web URL for this function) Enable tags:\nYou can add tags if you want: Key: Project, Value: RPA-Workshop Key: Function, Value: DocumentProcessor Step 7: Create the Function Review your settings: Function name: document-processor Runtime: Python 3.9 Execution role: RPA-Lambda-Role Click Create function Wait for the function to be created (usually takes 10-20 seconds) Step 8: Verify Function Creation You should see the Lambda function dashboard At the top, you\u0026rsquo;ll see: \u0026ldquo;Successfully created the function document-processor\u0026rdquo; The function status should show as Active Step 9: Explore the Function Interface Code tab:\nYou\u0026rsquo;ll see a default Python code template We\u0026rsquo;ll replace this with our document processing code in the next step Configuration tab:\nShows runtime settings, memory, timeout, etc. Default timeout is 3 seconds (we\u0026rsquo;ll increase this later) Monitoring tab:\nWill show metrics once the function starts running Currently empty since we haven\u0026rsquo;t run it yet The function name document-processor will appear in CloudWatch logs - remember this for debugging!\n"
},
{
	"uri": "//localhost:1313/2-document-bot/2.2-add-code/",
	"title": "Add Processing Code",
	"tags": [],
	"description": "",
	"content": "Overview Add Python code to the Lambda function that will use Amazon Textract to extract text and data from uploaded documents.\nStep-by-Step Instructions Step 1: Open the Code Editor In your document-processor Lambda function Make sure you\u0026rsquo;re on the Code tab You\u0026rsquo;ll see the default code in the editor Step 2: Clear Default Code Select all the default code in the editor Delete it completely The editor should be empty Step 3: Add the Document Processing Code Copy and paste this complete code:\nimport json import boto3 import uuid from datetime import datetime from decimal import Decimal # Initialize AWS clients textract = boto3.client(\u0026#39;textract\u0026#39;) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) s3 = boto3.client(\u0026#39;s3\u0026#39;) def lambda_handler(event, context): \u0026#34;\u0026#34;\u0026#34; Process documents uploaded to S3 using Textract \u0026#34;\u0026#34;\u0026#34; try: # Get S3 event details bucket = event[\u0026#39;Records\u0026#39;][0][\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = event[\u0026#39;Records\u0026#39;][0][\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] print(f\u0026#34;Processing document: {key} from bucket: {bucket}\u0026#34;) # Skip if file is not a document if not is_document_file(key): print(f\u0026#34;Skipping non-document file: {key}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;message\u0026#39;: \u0026#39;File skipped - not a document\u0026#39;}) } # Use Textract to analyze the document response = textract.analyze_document( Document={ \u0026#39;S3Object\u0026#39;: { \u0026#39;Bucket\u0026#39;: bucket, \u0026#39;Name\u0026#39;: key } }, FeatureTypes=[\u0026#39;FORMS\u0026#39;, \u0026#39;TABLES\u0026#39;] ) # Extract text and key-value pairs extracted_data = extract_document_data(response) # Store results in DynamoDB table = dynamodb.Table(\u0026#39;rpa-processed-data\u0026#39;) document_id = str(uuid.uuid4()) table.put_item( Item={ \u0026#39;document_id\u0026#39;: document_id, \u0026#39;file_name\u0026#39;: key, \u0026#39;bucket_name\u0026#39;: bucket, \u0026#39;extracted_text\u0026#39;: extracted_data[\u0026#39;text\u0026#39;][:1000], # Limit text length \u0026#39;key_value_pairs\u0026#39;: extracted_data[\u0026#39;key_values\u0026#39;], \u0026#39;processed_at\u0026#39;: datetime.utcnow().isoformat(), \u0026#39;status\u0026#39;: \u0026#39;completed\u0026#39;, \u0026#39;confidence_score\u0026#39;: Decimal(str(extracted_data[\u0026#39;confidence\u0026#39;])) # Convert to Decimal for DynamoDB } ) print(f\u0026#34;Successfully processed document: {key}\u0026#34;) print(f\u0026#34;Document ID: {document_id}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;message\u0026#39;: \u0026#39;Document processed successfully\u0026#39;, \u0026#39;document_id\u0026#39;: document_id, \u0026#39;file\u0026#39;: key, \u0026#39;extracted_items\u0026#39;: len(extracted_data[\u0026#39;key_values\u0026#39;]) }) } except Exception as e: print(f\u0026#34;Error processing document: {str(e)}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;error\u0026#39;: str(e), \u0026#39;message\u0026#39;: \u0026#39;Failed to process document\u0026#39; }) } def is_document_file(filename): \u0026#34;\u0026#34;\u0026#34; Check if the file is a document that Textract can process \u0026#34;\u0026#34;\u0026#34; valid_extensions = [\u0026#39;.pdf\u0026#39;, \u0026#39;.png\u0026#39;, \u0026#39;.jpg\u0026#39;, \u0026#39;.jpeg\u0026#39;, \u0026#39;.tiff\u0026#39;, \u0026#39;.tif\u0026#39;] return any(filename.lower().endswith(ext) for ext in valid_extensions) def extract_document_data(textract_response): \u0026#34;\u0026#34;\u0026#34; Extract text and key-value pairs from Textract response \u0026#34;\u0026#34;\u0026#34; blocks = textract_response[\u0026#39;Blocks\u0026#39;] # Extract all text text_blocks = [block[\u0026#39;Text\u0026#39;] for block in blocks if block[\u0026#39;BlockType\u0026#39;] == \u0026#39;LINE\u0026#39;] full_text = \u0026#39; \u0026#39;.join(text_blocks) # Extract key-value pairs key_values = {} key_map = {} value_map = {} block_map = {} # Build block maps for block in blocks: block_id = block[\u0026#39;Id\u0026#39;] block_map[block_id] = block if block[\u0026#39;BlockType\u0026#39;] == \u0026#34;KEY_VALUE_SET\u0026#34;: if \u0026#39;KEY\u0026#39; in block[\u0026#39;EntityTypes\u0026#39;]: key_map[block_id] = block else: value_map[block_id] = block # Match keys with values for key_block_id, key_block in key_map.items(): value_block = find_value_block(key_block, value_map) if value_block: key_text = get_text(key_block, block_map).strip() value_text = get_text(value_block, block_map).strip() # Only store non-empty key-value pairs if key_text and value_text: key_values[key_text] = value_text # Calculate average confidence confidences = [block.get(\u0026#39;Confidence\u0026#39;, 0) for block in blocks if \u0026#39;Confidence\u0026#39; in block] avg_confidence = sum(confidences) / len(confidences) if confidences else 0 return { \u0026#39;text\u0026#39;: full_text, \u0026#39;key_values\u0026#39;: key_values, \u0026#39;confidence\u0026#39;: round(avg_confidence, 2) } def find_value_block(key_block, value_map): \u0026#34;\u0026#34;\u0026#34;Find the value block associated with a key block\u0026#34;\u0026#34;\u0026#34; for relationship in key_block.get(\u0026#39;Relationships\u0026#39;, []): if relationship[\u0026#39;Type\u0026#39;] == \u0026#39;VALUE\u0026#39;: for value_id in relationship[\u0026#39;Ids\u0026#39;]: return value_map.get(value_id) return None def get_text(result, blocks_map): \u0026#34;\u0026#34;\u0026#34;Extract text from a block\u0026#34;\u0026#34;\u0026#34; text = \u0026#39;\u0026#39; if \u0026#39;Relationships\u0026#39; in result: for relationship in result[\u0026#39;Relationships\u0026#39;]: if relationship[\u0026#39;Type\u0026#39;] == \u0026#39;CHILD\u0026#39;: for child_id in relationship[\u0026#39;Ids\u0026#39;]: child = blocks_map.get(child_id, {}) if child.get(\u0026#39;BlockType\u0026#39;) == \u0026#39;WORD\u0026#39;: text += child.get(\u0026#39;Text\u0026#39;, \u0026#39;\u0026#39;) + \u0026#39; \u0026#39; elif child.get(\u0026#39;BlockType\u0026#39;) == \u0026#39;SELECTION_ELEMENT\u0026#39;: if child.get(\u0026#39;SelectionStatus\u0026#39;) == \u0026#39;SELECTED\u0026#39;: text += \u0026#39;X \u0026#39; return text.strip() Step 4: Deploy the Code After pasting the code, click Deploy Wait for the \u0026ldquo;Changes deployed\u0026rdquo; message The code is now saved and ready to run Step 5: Configure Function Settings Click on the Configuration tab Click General configuration → Edit Change these settings: Timeout: 5 minutes (300 seconds) Memory: 512 MB Click Save Step 6: Test Code Syntax Go back to the Code tab Look for any red error indicators in the code If you see errors, double-check the code was pasted correctly Understanding the Code Main Function (lambda_handler):\nReceives S3 events when files are uploaded Calls Textract to analyze documents Stores results in DynamoDB Document Validation (is_document_file):\nChecks if uploaded file is a supported document type Prevents processing of non-document files Data Extraction (extract_document_data):\nParses Textract response Extracts plain text and key-value pairs Calculates confidence scores Helper Functions:\nfind_value_block: Links keys to their values get_text: Extracts text from Textract blocks Troubleshooting If deploy fails:\nCheck for syntax errors (red underlines) Make sure all brackets and quotes are matched Try copying the code again If timeout errors occur later:\nIncrease timeout to 10 minutes for large documents Consider increasing memory to 1024 MB If you see import errors:\nThe boto3 library is pre-installed in Lambda No additional packages needed for this code The code includes error handling and logging - check CloudWatch Logs if something goes wrong!\n"
},
{
	"uri": "//localhost:1313/2-document-bot/2.3-configure-trigger/",
	"title": "Configure S3 Trigger",
	"tags": [],
	"description": "",
	"content": "Overview Set up S3 to automatically trigger our Lambda function whenever a document is uploaded to the bucket.\nStep-by-Step Instructions Step 1: Add Trigger to Lambda Function In your document-processor Lambda function Scroll down to the Function overview section Click Add trigger button Step 2: Select Trigger Source In the trigger configuration page Click the Select a source dropdown Choose S3 from the list Step 3: Configure S3 Trigger Settings Bucket:\nSelect your bucket: rpa-documents-[your-name] This is the bucket you created in Step 1 Event type:\nSelect All object create events This triggers on any file upload (PUT, POST, COPY) Prefix:\nEnter: documents/ This means only files in the documents/ folder will trigger the function Leave empty if you want all uploads to trigger Step 4: Acknowledge Permissions You\u0026rsquo;ll see a checkbox: I acknowledge that using the same S3 bucket for both input and output is not recommended Check this box (we\u0026rsquo;re not using the same bucket for output) This is just a warning about best practices Step 5: Add the Trigger Review your settings: Source: S3 Bucket: rpa-documents-[your-name] Event type: All object create events Prefix: documents/ Click Add button Step 6: Verify Trigger Creation You should see a success message In the Function overview diagram, you\u0026rsquo;ll see: S3 bucket connected to your Lambda function The trigger is now active Step 7: Check S3 Bucket Configuration Go to S3 in the AWS console Open your rpa-documents-[your-name] bucket Click the Properties tab Scroll down to Event notifications You should see a new event notification configured Understanding S3 Event Triggers How it works:\nFile uploaded to S3 bucket S3 generates an event notification Event automatically invokes Lambda function Lambda receives event with bucket and file details Event types available:\nAll object create events: Any upload method PUT: Direct file uploads POST: Form-based uploads COPY: Files copied within S3 Prefix and Suffix filtering:\nPrefix: Only files in specific folders Suffix: Only files with specific extensions Both: Can be combined for precise filtering Troubleshooting If trigger creation fails:\nCheck that Lambda function has proper permissions Verify S3 bucket exists and you have access Make sure bucket name is spelled correctly If you don\u0026rsquo;t see the trigger in Function overview:\nRefresh the Lambda console page Check that the trigger was actually created Look for error messages in the console If S3 event notification doesn\u0026rsquo;t appear:\nGo to S3 bucket Properties tab Check Event notifications section The notification should show Lambda function as destination Common permission issues:\nLambda execution role needs S3 permissions (already configured) S3 needs permission to invoke Lambda (automatically configured) Testing the Trigger (Preview) Once configured, the trigger works like this:\nUpload a document to s3://your-bucket/documents/ S3 automatically calls your Lambda function Lambda processes the document with Textract Results are stored in DynamoDB You can see logs in CloudWatch What\u0026rsquo;s Next? Your S3 trigger is configured and ready! Next, we\u0026rsquo;ll test the complete document processing workflow by uploading a real document and verifying it gets processed automatically.\nThe documents/ prefix helps organize your files and prevents accidental processing of non-document files!\n"
},
{
	"uri": "//localhost:1313/2-document-bot/2.4-test-function/",
	"title": "Test Document Processing",
	"tags": [],
	"description": "",
	"content": "Overview Test the complete document processing workflow by uploading a real document and verifying it gets processed automatically.\nStep-by-Step Instructions Step 1: Prepare a Test Document Create a simple PDF\nOpen any word processor (Word, Google Docs, etc.) Create a simple document with: INVOICE Invoice Number: INV-001 Date: January 21, 2025 Customer: John Smith Amount: $100.00 Description: RPA Workshop Test Save as PDF: test-invoice.pdf Step 2: Upload Document to S3 Go to S3 in AWS Console Open your bucket: rpa-documents-[your-name] Click on the documents folder Click Upload Click Add files and select your test document Click Upload to start the process Step 3: Monitor Lambda Execution Go to AWS Lambda console Click on your document-processor function Click the Monitor tab You should see: Recent invocations increase by 1 Duration of the execution No errors (hopefully!) Step 4: Check CloudWatch Logs In the Monitor tab, click View CloudWatch logs Click on the most recent log stream Look for log entries like: Processing document: test-invoice.pdf from bucket: rpa-documents-yourname Successfully processed document: test-invoice.pdf Document ID: [some-uuid] Step 5: Verify Data in DynamoDB Go to DynamoDB in AWS Console Click Tables → rpa-processed-data Click Explore table items You should see a new item with: document_id: Unique identifier file_name: Your uploaded file name extracted_text: Text extracted from document key_value_pairs: Form fields found processed_at: Timestamp status: \u0026ldquo;completed\u0026rdquo; Step 6: Review Extracted Data Click on the DynamoDB item to see details:\nextracted_text:\nShould contain the main text from your document May be truncated to 1000 characters key_value_pairs:\nShould show form fields like: { \u0026#34;Invoice Number\u0026#34;: \u0026#34;INV-001\u0026#34;, \u0026#34;Date\u0026#34;: \u0026#34;January 21, 2025\u0026#34;, \u0026#34;Customer\u0026#34;: \u0026#34;John Smith\u0026#34;, \u0026#34;Amount\u0026#34;: \u0026#34;$100.00\u0026#34; } confidence_score:\nNumber between 0-100 indicating Textract\u0026rsquo;s confidence Higher is better (usually 80+ for clear documents) Step 7: Test with Different Document Types Try uploading different types of documents:\nTest 2: Image file\nTake a photo of a receipt or form Upload as JPG/PNG to documents/ folder Check if it processes correctly Test 3: Complex PDF\nUpload a multi-page document Check processing time and results What\u0026rsquo;s Next? Congratulations! Your document processing bot is working! The extracted data is now stored in DynamoDB and ready to be used by other parts of your RPA system.\nNext, we\u0026rsquo;ll create the email automation bot that will send you beautiful reports about the processed documents.\nSave some test documents - you\u0026rsquo;ll need them to test the complete system later!\n"
},
{
	"uri": "//localhost:1313/3-email-bot/",
	"title": "Build Email Automation Bot",
	"tags": [],
	"description": "",
	"content": "Overview Create a Lambda function that sends automated email notifications when documents are processed, using Amazon SES.\nWhat You\u0026rsquo;ll Build Lambda function for email automation Integration with SES for sending emails Automatic notifications based on document processing results Expected Result Email automation bot sends processing summaries Beautiful HTML emails with document data Optional daily scheduled reports Integration with document processing workflow You can customize the email template to include more details or change the styling!\n"
},
{
	"uri": "//localhost:1313/3-email-bot/3.1-create-email-lambda/",
	"title": "Create Email Lambda Function",
	"tags": [],
	"description": "",
	"content": "Overview Create a second Lambda function that will send automated email notifications with processing summaries from our RPA system.\nStep-by-Step Instructions Step 1: Navigate to AWS Lambda In the AWS Management Console Go to AWS Lambda (you should already be there from the previous step) You\u0026rsquo;ll see your existing document-processor function in the list Step 2: Create Another Function Click the Create function button again We\u0026rsquo;re creating a second Lambda function for email automation Step 3: Configure Basic Information Function creation method:\nSelect Author from scratch Function name:\nEnter: email-automation-bot This name clearly indicates it\u0026rsquo;s for email automation Runtime:\nSelect Python 3.9 (same as the first function) Architecture:\nKeep x86_64 (default) Step 4: Configure Permissions Execution role:\nSelect Use an existing role Choose: RPA-Lambda-Role (same role as before) This role already has SES and DynamoDB permissions we need Step 5: Optional Settings Tags (recommended):\nKey: Project, Value: RPA-Workshop Key: Function, Value: EmailBot Key: Type, Value: Automation Step 6: Create the Function Review your settings: Function name: email-automation-bot Runtime: Python 3.9 Execution role: RPA-Lambda-Role Click Create function Wait for creation to complete Step 7: Verify Function Creation You should see the success message The function status should be Active You now have 2 Lambda functions in your account Step 8: Configure Function Settings Click on the Configuration tab Click General configuration → Edit Update these settings: Timeout: 2 minutes (120 seconds) Memory: 256 MB (less than document processor) Click Save Step 9: Verify Both Functions Exist Go back to the Lambda functions list You should see both functions: ✅ document-processor (for processing documents) ✅ email-automation-bot (for sending emails) Understanding the Email Bot Function Purpose:\nSend processing summary emails Generate HTML reports with extracted data Provide notifications about RPA system status Why a separate function?\nDifferent trigger mechanism (scheduled vs. S3 events) Different resource requirements (less memory needed) Easier to manage and debug separately Can be scheduled independently Resource Requirements:\nLess memory than document processor (256 MB vs 512 MB) Shorter timeout (2 minutes vs 5 minutes) Mainly does database queries and email sending Troubleshooting If function creation fails:\nMake sure you\u0026rsquo;re using the same IAM role: RPA-Lambda-Role Check that the function name doesn\u0026rsquo;t conflict Verify you have Lambda creation permissions If you can\u0026rsquo;t see both functions:\nRefresh the Lambda console page Make sure you\u0026rsquo;re in the correct AWS region Check the function list filters If configuration fails:\nTry setting timeout and memory one at a time Make sure values are within AWS limits Refresh and try again if needed Function Comparison Feature document-processor email-automation-bot Purpose Process documents Send email reports Trigger S3 upload events Scheduled/manual Memory 512 MB 256 MB Timeout 5 minutes 2 minutes Main Services Textract, DynamoDB SES, DynamoDB What\u0026rsquo;s Next? Your email automation Lambda function is ready! Next, we\u0026rsquo;ll add the Python code that will:\nQuery processed documents from DynamoDB Generate beautiful HTML email reports Send notifications via Amazon SES Having separate functions for different tasks is a best practice in serverless architecture!\n"
},
{
	"uri": "//localhost:1313/3-email-bot/3.2-add-email-code/",
	"title": "Add Email Automation Code",
	"tags": [],
	"description": "",
	"content": "Overview Add Python code to the email Lambda function that will query processed documents and send beautiful HTML email reports.\nStep-by-Step Instructions Step 1: Open Email Function Code Editor In your email-automation-bot Lambda function Make sure you\u0026rsquo;re on the Code tab You\u0026rsquo;ll see the default Lambda code template Step 2: Clear Default Code Select all the default code in the editor Delete it completely The editor should be empty Step 3: Add Email Automation Code Copy and paste this complete code:\nimport json import boto3 from datetime import datetime, timedelta # Initialize AWS clients ses = boto3.client(\u0026#39;ses\u0026#39;) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) def lambda_handler(event, context): \u0026#34;\u0026#34;\u0026#34; Send email notifications for processed documents \u0026#34;\u0026#34;\u0026#34; try: # Get document processing results from DynamoDB table = dynamodb.Table(\u0026#39;rpa-processed-data\u0026#39;) # Get documents from the last 24 hours cutoff_time = (datetime.utcnow() - timedelta(hours=24)).isoformat() response = table.scan( FilterExpression=\u0026#39;processed_at \u0026gt; :cutoff_time\u0026#39;, ExpressionAttributeValues={\u0026#39;:cutoff_time\u0026#39;: cutoff_time} ) recent_documents = response[\u0026#39;Items\u0026#39;] # If no recent documents, send a status email if not recent_documents: recent_documents = get_latest_documents(table, 5) # Get last 5 documents # Generate email content email_content = generate_email_content(recent_documents) # Get recipient email from environment or use default recipient_email = \u0026#34;YOUR_EMAIL@example.com\u0026#34; # Replace with your verified email # Send email using SES send_email( subject=\u0026#34;🤖 RPA Processing Summary - \u0026#34; + datetime.utcnow().strftime(\u0026#39;%Y-%m-%d\u0026#39;), body=email_content, recipient=recipient_email ) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;message\u0026#39;: \u0026#39;Email sent successfully\u0026#39;, \u0026#39;documents_processed\u0026#39;: len(recent_documents), \u0026#39;recipient\u0026#39;: recipient_email }) } except Exception as e: print(f\u0026#34;Error sending email: {str(e)}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;error\u0026#39;: str(e), \u0026#39;message\u0026#39;: \u0026#39;Failed to send email\u0026#39; }) } def get_latest_documents(table, limit=5): \u0026#34;\u0026#34;\u0026#34; Get the most recent documents if no recent ones found \u0026#34;\u0026#34;\u0026#34; try: response = table.scan() items = response[\u0026#39;Items\u0026#39;] # Sort by processed_at timestamp (most recent first) sorted_items = sorted(items, key=lambda x: x.get(\u0026#39;processed_at\u0026#39;, \u0026#39;\u0026#39;), reverse=True) return sorted_items[:limit] except Exception as e: print(f\u0026#34;Error getting latest documents: {str(e)}\u0026#34;) return [] def generate_email_content(documents): \u0026#34;\u0026#34;\u0026#34; Generate HTML email content from processed documents \u0026#34;\u0026#34;\u0026#34; html_content = f\u0026#34;\u0026#34;\u0026#34; \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;style\u0026gt; body {{ font-family: \u0026#39;Segoe UI\u0026#39;, Tahoma, Geneva, Verdana, sans-serif; line-height: 1.6; color: #333; max-width: 800px; margin: 0 auto; padding: 20px; }} .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 10px; text-align: center; margin-bottom: 30px; }} .header h1 {{ margin: 0; font-size: 28px; }} .header p {{ margin: 10px 0 0 0; opacity: 0.9; }} .summary {{ background: #f8f9fa; padding: 20px; border-radius: 8px; margin-bottom: 30px; border-left: 4px solid #667eea; }} .document {{ background: white; border: 1px solid #e9ecef; margin: 15px 0; padding: 20px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }} .document h3 {{ color: #495057; margin-top: 0; border-bottom: 2px solid #e9ecef; padding-bottom: 10px; }} .key-value {{ margin: 8px 0; padding: 5px 0; }} .key {{ font-weight: 600; color: #495057; display: inline-block; min-width: 120px; }} .value {{ color: #6c757d; background: #f8f9fa; padding: 2px 8px; border-radius: 4px; }} .confidence {{ background: #d4edda; color: #155724; padding: 4px 8px; border-radius: 4px; font-size: 12px; font-weight: bold; }} .footer {{ margin-top: 40px; padding: 20px; background: #e9ecef; border-radius: 8px; text-align: center; color: #6c757d; }} .stats {{ display: flex; justify-content: space-around; margin: 20px 0; }} .stat {{ text-align: center; padding: 15px; }} .stat-number {{ font-size: 24px; font-weight: bold; color: #667eea; }} .stat-label {{ font-size: 12px; color: #6c757d; text-transform: uppercase; }} \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;header\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;🤖 RPA Processing Summary\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Generated on: {datetime.utcnow().strftime(\u0026#39;%B %d, %Y at %H:%M UTC\u0026#39;)}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;summary\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;stats\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;stat\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;stat-number\u0026#34;\u0026gt;{len(documents)}\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;stat-label\u0026#34;\u0026gt;Documents Processed\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;stat\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;stat-number\u0026#34;\u0026gt;{calculate_avg_confidence(documents):.1f}%\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;stat-label\u0026#34;\u0026gt;Avg Confidence\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;stat\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;stat-number\u0026#34;\u0026gt;{count_total_extractions(documents)}\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;stat-label\u0026#34;\u0026gt;Data Points Extracted\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026#34;\u0026#34;\u0026#34; if not documents: html_content += \u0026#34;\u0026#34;\u0026#34; \u0026lt;div class=\u0026#34;document\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;📭 No Recent Documents\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;No documents have been processed in the last 24 hours. Your RPA system is ready and waiting for new documents!\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026#34;\u0026#34;\u0026#34; else: for i, doc in enumerate(documents[:10]): # Show max 10 documents file_name = doc.get(\u0026#39;file_name\u0026#39;, \u0026#39;Unknown File\u0026#39;) processed_at = doc.get(\u0026#39;processed_at\u0026#39;, \u0026#39;Unknown\u0026#39;) confidence = doc.get(\u0026#39;confidence_score\u0026#39;, 0) # Format timestamp try: dt = datetime.fromisoformat(processed_at.replace(\u0026#39;Z\u0026#39;, \u0026#39;+00:00\u0026#39;)) formatted_time = dt.strftime(\u0026#39;%B %d, %Y at %H:%M UTC\u0026#39;) except: formatted_time = processed_at html_content += f\u0026#34;\u0026#34;\u0026#34; \u0026lt;div class=\u0026#34;document\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;📄 {file_name}\u0026lt;/h3\u0026gt; \u0026lt;div class=\u0026#34;key-value\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;key\u0026#34;\u0026gt;Processed:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;value\u0026#34;\u0026gt;{formatted_time}\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;key-value\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;key\u0026#34;\u0026gt;Confidence:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;confidence\u0026#34;\u0026gt;{confidence}%\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026#34;\u0026#34;\u0026#34; # Add extracted key-value pairs key_values = doc.get(\u0026#39;key_value_pairs\u0026#39;, {}) if key_values and isinstance(key_values, dict): html_content += \u0026#34;\u0026lt;h4\u0026gt;📊 Extracted Data:\u0026lt;/h4\u0026gt;\u0026#34; for key, value in list(key_values.items())[:5]: # Show first 5 key-value pairs html_content += f\u0026#34;\u0026#34;\u0026#34; \u0026lt;div class=\u0026#34;key-value\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;key\u0026#34;\u0026gt;{key}:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;value\u0026#34;\u0026gt;{value}\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026#34;\u0026#34;\u0026#34; html_content += \u0026#34;\u0026lt;/div\u0026gt;\u0026#34; html_content += f\u0026#34;\u0026#34;\u0026#34; \u0026lt;div class=\u0026#34;footer\u0026#34;\u0026gt; \u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;✅ RPA System Status: Operational\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Your automated document processing system is running smoothly.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;This is an automated message from your RPA platform on AWS.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026#34;\u0026#34;\u0026#34; return html_content def calculate_avg_confidence(documents): \u0026#34;\u0026#34;\u0026#34;Calculate average confidence score\u0026#34;\u0026#34;\u0026#34; if not documents: return 0 confidences = [doc.get(\u0026#39;confidence_score\u0026#39;, 0) for doc in documents] valid_confidences = [c for c in confidences if c \u0026gt; 0] return sum(valid_confidences) / len(valid_confidences) if valid_confidences else 0 def count_total_extractions(documents): \u0026#34;\u0026#34;\u0026#34;Count total data points extracted\u0026#34;\u0026#34;\u0026#34; total = 0 for doc in documents: key_values = doc.get(\u0026#39;key_value_pairs\u0026#39;, {}) if isinstance(key_values, dict): total += len(key_values) return total def send_email(subject, body, recipient): \u0026#34;\u0026#34;\u0026#34; Send email using Amazon SES \u0026#34;\u0026#34;\u0026#34; try: response = ses.send_email( Source=\u0026#39;YOUR_EMAIL@example.com\u0026#39;, # Replace with your verified email Destination={ \u0026#39;ToAddresses\u0026#39;: [recipient] }, Message={ \u0026#39;Subject\u0026#39;: { \u0026#39;Data\u0026#39;: subject, \u0026#39;Charset\u0026#39;: \u0026#39;UTF-8\u0026#39; }, \u0026#39;Body\u0026#39;: { \u0026#39;Html\u0026#39;: { \u0026#39;Data\u0026#39;: body, \u0026#39;Charset\u0026#39;: \u0026#39;UTF-8\u0026#39; } } } ) print(f\u0026#34;Email sent successfully. Message ID: {response[\u0026#39;MessageId\u0026#39;]}\u0026#34;) return response except Exception as e: print(f\u0026#34;Error sending email: {str(e)}\u0026#34;) raise e Step 4: Update Email Addresses IMPORTANT: You must update the email addresses in the code:\nFind this line: recipient_email = \u0026quot;YOUR_EMAIL@example.com\u0026quot; Replace with your verified email from Step 1.3 Find this line: Source='YOUR_EMAIL@example.com' Replace with the same verified email Both should be the same email address you verified in SES Step 5: Deploy the Code After updating email addresses, click Deploy Wait for the \u0026ldquo;Changes deployed\u0026rdquo; message The code is now saved and ready to run Step 6: Test the Function Click the Test button Select Create new test event Event name: test-email Keep the default JSON (the function doesn\u0026rsquo;t use event data) Click Test Check your email for the RPA summary report Understanding the Email Code Main Features:\nQueries DynamoDB for recent documents Generates professional HTML emails Includes statistics and summaries Handles cases with no recent documents Responsive email design Email Content:\nProcessing statistics Individual document details Extracted data preview Confidence scores Professional styling Troubleshooting If email doesn\u0026rsquo;t arrive:\nCheck spam/junk folder Verify email addresses are correct Check SES sending limits Look at CloudWatch logs for errors If function fails:\nCheck email addresses are verified in SES Verify DynamoDB table permissions Check for syntax errors in code What\u0026rsquo;s Next? Your email automation code is ready! Next, we\u0026rsquo;ll set up automatic scheduling so you receive daily reports without manual intervention.\nThe email includes responsive design and works well on both desktop and mobile devices!\n"
},
{
	"uri": "//localhost:1313/3-email-bot/3.3-configure-schedule/",
	"title": "Configure Email Schedule",
	"tags": [],
	"description": "",
	"content": "Overview Set up automatic scheduling for your email automation bot using Amazon EventBridge (CloudWatch Events) to receive daily RPA reports.\nStep-by-Step Instructions Step 1: Add EventBridge Trigger In your email-automation-bot Lambda function Scroll down to the Function overview section Click Add trigger button Step 2: Select EventBridge as Source In the trigger configuration page Click the Select a source dropdown Choose EventBridge (CloudWatch Events) from the list Step 3: Configure EventBridge Rule Rule:\nSelect Create a new rule This will create a new scheduling rule specifically for your email bot Rule name:\nEnter: daily-rpa-email-schedule This name describes what the rule does Rule description:\nEnter: Daily schedule for RPA email reports Optional but helpful for documentation Step 4: Set Schedule Expression Rule type:\nSelect Schedule expression This allows us to set up recurring schedules Schedule expression:\nEnter: rate(1 day) This means the function will run once every day Alternative schedule options:\nrate(12 hours) - Every 12 hours rate(1 hour) - Every hour (for testing) cron(0 9 * * ? *) - Every day at 9:00 AM UTC cron(0 17 * * MON-FRI *) - Weekdays at 5:00 PM UTC Step 5: Configure Target Target:\nShould automatically show your Lambda function If not, select Lambda function and choose email-automation-bot Configure input:\nSelect Constant (JSON text) Enter this JSON: { \u0026#34;source\u0026#34;: \u0026#34;scheduled-event\u0026#34;, \u0026#34;detail-type\u0026#34;: \u0026#34;Daily RPA Report\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;daily\u0026#34; } Step 6: Add the Trigger Review your settings: Source: EventBridge (CloudWatch Events) Rule: daily-rpa-email-schedule Schedule: rate(1 day) Target: email-automation-bot Click Add button Step 7: Verify Trigger Creation You should see a success message In the Function overview diagram, you\u0026rsquo;ll see: EventBridge connected to your Lambda function The schedule trigger is now active Step 9: Check EventBridge Console Go to Amazon EventBridge in AWS Console Click Rules in the left menu You should see your rule: daily-rpa-email-schedule Click on it to see details and modify if needed Understanding EventBridge Scheduling Schedule Expressions:\nRate expressions: rate(value unit)\nUnits: minute, minutes, hour, hours, day, days Examples: rate(5 minutes), rate(1 hour), rate(7 days) Cron expressions: cron(minute hour day month day-of-week year)\nMore precise timing control Examples: cron(0 12 * * ? *) (daily at noon UTC) Time Zones:\nAll schedules use UTC time Consider your local timezone when setting schedules 9 AM EST = 2 PM UTC (during standard time) Schedule Recommendations For Production:\nrate(1 day) - Daily summary (recommended) cron(0 9 * * ? *) - Daily at 9 AM UTC cron(0 17 * * MON-FRI *) - Weekdays at 5 PM UTC For Testing:\nrate(5 minutes) - Every 5 minutes (delete after testing) rate(1 hour) - Hourly (for short-term testing) Troubleshooting If trigger creation fails:\nCheck Lambda function permissions Verify EventBridge service is available in your region Try a different rule name if there\u0026rsquo;s a conflict If emails don\u0026rsquo;t send on schedule:\nCheck CloudWatch logs for the scheduled executions Verify the Lambda function runs without errors Check SES sending limits and quotas If you want to change the schedule:\nGo to EventBridge console Find your rule: daily-rpa-email-schedule Click Edit to modify the schedule expression Managing the Schedule To temporarily disable:\nGo to EventBridge console Find your rule and click Disable Re-enable when ready To delete the schedule:\nIn Lambda function, find the EventBridge trigger Click the trigger and select Delete Or delete from EventBridge console Cost Considerations EventBridge pricing:\nFirst 14 million events per month: Free Additional events: $1.00 per million events Daily emails = ~30 events per month (well within free tier) Lambda pricing:\nScheduled executions count toward your Lambda usage Daily execution = ~30 invocations per month Minimal cost impact What\u0026rsquo;s Next? Your email automation is now fully automated! The system will:\nRun daily at the scheduled time Query processed documents from the last 24 hours Generate and send beautiful HTML email reports Continue running without manual intervention Next, we\u0026rsquo;ll test the complete system end-to-end to make sure everything works together perfectly.\nStart with daily reports, then adjust the frequency based on your document processing volume!\n"
},
{
	"uri": "//localhost:1313/3-email-bot/3.4-test-email/",
	"title": "Test Email System",
	"tags": [],
	"description": "",
	"content": "Overview Test the complete email automation system to ensure it generates and sends beautiful reports with your processed document data.\nStep-by-Step Instructions Step 1: Manual Function Test In your email-automation-bot Lambda function Click the Test button in the code editor If you haven\u0026rsquo;t created a test event yet: Select Create new test event Event name: manual-email-test Keep the default JSON template Click Test to execute Step 2: Monitor Test Execution Watch the Execution result section Look for: Status: Succeeded (green) Duration: Should be under 10 seconds Logs: Should show email sending process Step 3: Check CloudWatch Logs Click Monitor tab in your Lambda function Click View CloudWatch logs Click the most recent log stream Look for log entries like: Email sent successfully. Message ID: [some-id] Step 4: Verify Email Delivery Check your email inbox (the one you verified in SES) Look for email with subject: \u0026ldquo;🤖 RPA Processing Summary - [date]\u0026rdquo; If not in inbox, check spam/junk folder Email should arrive within 1-2 minutes Step 5: Review Email Content Open the email and verify it contains:\nHeader Section:\nProfessional gradient header \u0026ldquo;RPA Processing Summary\u0026rdquo; title Generation timestamp Statistics Section:\nNumber of documents processed Average confidence score Total data points extracted Document Details:\nIndividual document entries File names and processing times Confidence scores Extracted key-value pairs (preview) Footer:\nSystem status indicator Professional closing message Step 6: Test with Recent Documents Upload a new document to your S3 bucket (documents/ folder) Wait for it to be processed (check DynamoDB) Run the email function test again Verify the new document appears in the email Step 7: Test Different Scenarios Test A: No recent documents\nDon\u0026rsquo;t upload any documents for 24+ hours Run email function Should show \u0026ldquo;No Recent Documents\u0026rdquo; message Should still include latest processed documents Test B: Multiple documents\nUpload 3-5 different documents Wait for all to be processed Run email function Should show all documents with statistics Step 8: Test Scheduled Execution Option A: Wait for scheduled time\nIf you set daily schedule, wait for next day Check email arrives automatically Option B: Create temporary hourly schedule\nAdd another EventBridge trigger with rate(1 hour) Wait 1 hour for automatic execution Check email arrives without manual trigger Delete the hourly trigger after testing Troubleshooting If email doesn\u0026rsquo;t arrive:\nCheck spam/junk folder first Verify email address is correct in code Check SES verified identities Look at CloudWatch logs for errors Check SES sending statistics If email content is wrong:\nVerify DynamoDB has processed documents Check document processing timestamps Look for errors in email generation code Test with different document types If formatting is broken:\nCheck HTML syntax in email template Verify CSS styles are properly closed Test email in different email clients Check for special characters in document data Common error messages:\n\u0026quot;MessageRejected\u0026quot;: Email address not verified \u0026quot;SendingQuotaExceeded\u0026quot;: SES daily limit reached \u0026quot;Throttling\u0026quot;: Sending too fast, retry later Performance Optimization Email size considerations:\nCurrent template shows max 10 documents Each document shows max 5 key-value pairs Total email size typically \u0026lt; 100KB Good balance between detail and performance Delivery optimization:\nEmails typically deliver in 30-60 seconds SES has high deliverability rates Professional formatting improves inbox placement Avoid spam trigger words in content Advanced Testing Load testing:\nProcess 20+ documents Run email function Verify performance and formatting Error handling testing:\nTemporarily break DynamoDB permissions Run email function Should handle errors gracefully Schedule reliability:\nMonitor scheduled executions for several days Verify consistent delivery times Check for any missed executions What\u0026rsquo;s Next? Congratulations! Your email automation system is working perfectly. You now have:\nAutomatic document processing with AI Beautiful email reports with extracted data Scheduled daily summaries Professional monitoring and notifications "
},
{
	"uri": "//localhost:1313/4-cleanup/",
	"title": "Cleanup Resources",
	"tags": [],
	"description": "",
	"content": "Overview Clean up all AWS resources created during this workshop to avoid ongoing charges.\nImportant: This will permanently delete all resources and data created during the workshop. Make sure to backup any important data before proceeding.\nStep 1: Delete Lambda Functions Go to AWS Lambda console Select and delete these functions: document-processor email-automation-bot Any other Lambda functions you created Step 2: Delete S3 Bucket Go to S3 console Select your bucket rpa-documents-[your-name] Click Empty to delete all objects Confirm by typing the bucket name Click Delete bucket Confirm deletion Step 3: Delete DynamoDB Table Go to DynamoDB console Select table rpa-processed-data Click Delete Confirm by typing delete Step 4: Delete IAM Role Go to IAM console Click Roles Search for RPA-Lambda-Role Select and click Delete Confirm deletion Step 5: Remove SES Verified Email Go to Amazon SES console Click Verified identities Select your email address Click Delete Step 6: Delete CloudWatch Logs Go to CloudWatch console Click Log groups Delete log groups starting with /aws/lambda/ Select and delete relevant log groups Step 7: Delete API Gateway (If Created) Go to API Gateway console Select your API (if you created one) Click Actions → Delete Step 8: Check EventBridge Rules Go to EventBridge console Click Rules Delete any rules you created (like daily-rpa-summary) Final Verification Check these services to ensure everything is deleted:\n✅ Lambda functions deleted ✅ S3 bucket deleted ✅ DynamoDB table deleted ✅ IAM role deleted ✅ CloudWatch logs deleted Cost Check Go to AWS Billing Dashboard Check Bills section for current month charges Most services used in this workshop are free tier eligible Any charges should be minimal (under $5) What You\u0026rsquo;ve Learned Congratulations! You\u0026rsquo;ve successfully:\n✅ Built a complete RPA platform with AI integration ✅ Used multiple AWS services together ✅ Implemented document processing automation ✅ Created email automation workflows ✅ Set up monitoring and logging ✅ Learned AWS best practices for cleanup "
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]